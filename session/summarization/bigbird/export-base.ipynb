{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/husein/t5/prepare/mesolitica-tpu.json'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigbird import modeling\n",
    "from bigbird import utils\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab = '/home/husein/b2b/sp10m.cased.t5.model'\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(vocab)\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, sp):\n",
    "        self.sp = sp\n",
    "        self.vocab_size = sp.GetPieceSize() + 100\n",
    "    \n",
    "    def encode(self, s):\n",
    "        return self.sp.EncodeAsIds(s)\n",
    "    \n",
    "    def decode(self, ids, strip_extraneous=False):\n",
    "        return self.sp.DecodeIds(list(ids))\n",
    "    \n",
    "encoder = Encoder(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = tf.placeholder(tf.float32, None, name = 'top_p')\n",
    "temperature = tf.placeholder(tf.float32, None, name = 'temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "    # transformer basic configs\n",
    "    'attention_probs_dropout_prob': 0.1,\n",
    "    'hidden_act': 'relu',\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "    'hidden_size': 768,\n",
    "    'initializer_range': 0.02,\n",
    "    'intermediate_size': 3072,\n",
    "    'max_position_embeddings': 4096,\n",
    "    'max_encoder_length': 2048,\n",
    "    'max_decoder_length': 1024,\n",
    "    'num_attention_heads': 12,\n",
    "    'num_hidden_layers': 12,\n",
    "    'type_vocab_size': 2,\n",
    "    'scope': 'pegasus',\n",
    "    'use_bias': False,\n",
    "    'rescale_embedding': True,\n",
    "    'vocab_model_file': None,\n",
    "    # sparse mask configs\n",
    "    'attention_type': 'block_sparse',\n",
    "    'norm_type': 'prenorm',\n",
    "    'block_size': 64,\n",
    "    'num_rand_blocks': 3,\n",
    "    'vocab_size': 32000,\n",
    "    'beam_size': 1,\n",
    "    'alpha': 0.0,\n",
    "    'couple_encoder_decoder': False,\n",
    "    'num_warmup_steps': 10000,\n",
    "    'learning_rate': 0.0001,\n",
    "    'label_smoothing': 0.1,\n",
    "    'optimizer': 'Adafactor',\n",
    "    'use_tpu': True,\n",
    "    'top_p': top_p,\n",
    "    'temperature': temperature\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modeling.TransformerModel(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya/Malaya/pretrained-model/bigbird/bigbird/modeling.py:226: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor 'pegasus/log_probs:0' shape=(?, 1024) dtype=float32>,\n",
       "  <tf.Tensor 'pegasus/logits:0' shape=(?, 1024, 32000) dtype=float32>,\n",
       "  <tf.Tensor 'pegasus/while/Exit_1:0' shape=(?, 1024) dtype=int32>),\n",
       " <tf.Tensor 'pegasus/encoder/LayerNorm/batchnorm/add_1:0' shape=(?, 2048, 768) dtype=float32>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = model(X, training = False)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logits:0' shape=(?, 1024) dtype=int32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.identity(r[0][2], name = 'logits')\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = tf.gfile.Glob('gs://mesolitica-tpu-general/t2t-summarization-v2/data/seq2*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4\n",
    "# data_fields = {\n",
    "#     'inputs': tf.VarLenFeature(tf.int64),\n",
    "#     'targets': tf.VarLenFeature(tf.int64),\n",
    "# }\n",
    "# data_len = {\n",
    "#     'inputs': 2048,\n",
    "#     'targets': 1024,\n",
    "# }\n",
    "\n",
    "# def parse(serialized_example):\n",
    "\n",
    "#     features = tf.parse_single_example(\n",
    "#         serialized_example, features = data_fields\n",
    "#     )\n",
    "#     for k in features.keys():\n",
    "#         features[k] = features[k].values\n",
    "#         features[k] = tf.pad(\n",
    "#             features[k], [[0, data_len[k] - tf.shape(features[k])[0]]]\n",
    "#         )\n",
    "#         features[k].set_shape((data_len[k]))\n",
    "\n",
    "#     return features\n",
    "\n",
    "# def _decode_record(example, name_to_features):\n",
    "#     \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "\n",
    "#     # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "#     # So cast all int64 to int32.\n",
    "#     for name in list(example.keys()):\n",
    "#         t = example[name]\n",
    "#         if t.dtype == tf.int64:\n",
    "#             t = tf.to_int32(t)\n",
    "#         example[name] = t\n",
    "\n",
    "#     return example\n",
    "\n",
    "# d = tf.data.TFRecordDataset(files)\n",
    "# d = d.map(parse, num_parallel_calls = 32)\n",
    "# d = d.apply(\n",
    "#     tf.contrib.data.map_and_batch(\n",
    "#         lambda record: _decode_record(record, data_fields),\n",
    "#         batch_size = batch_size,\n",
    "#         num_parallel_batches = 4,\n",
    "#         drop_remainder = True,\n",
    "#     )\n",
    "# )\n",
    "# d = d.make_one_shot_iterator().get_next()\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://mesolitica-tpu-general/bigbird-summarization-base/model.ckpt-500000'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "ckpt_path = tf.train.latest_checkpoint('gs://mesolitica-tpu-general/bigbird-summarization-base')\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_ = sess.run(d)\n",
    "# encoder.decode(r_['inputs'][2].tolist())\n",
    "# encoder.decode(r_['targets'][2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import collections\n",
    "\n",
    "# def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "#     \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
    "#     assignment_map = {}\n",
    "#     initialized_variable_names = {}\n",
    "\n",
    "#     name_to_variable = collections.OrderedDict()\n",
    "#     for var in tvars:\n",
    "#         name = var.name\n",
    "#         m = re.match('^(.*):\\\\d+$', name)\n",
    "#         if m is not None:\n",
    "#             name = m.group(1)\n",
    "#         name_to_variable[name] = var\n",
    "\n",
    "#     init_vars = tf.train.list_variables(init_checkpoint)\n",
    "#     assignment_map = collections.OrderedDict()\n",
    "#     for x in init_vars:\n",
    "#         (name, var) = (x[0], x[1])\n",
    "\n",
    "#         l = 'pegasus/' + name\n",
    "#         l = l.replace('embeddings/weights', 'embeddings/word_embeddings')\n",
    "#         l = l.replace('self/output', 'output')\n",
    "#         l = l.replace('ffn/dense_1', 'output/dense')\n",
    "#         l = l.replace('ffn', 'intermediate')\n",
    "#         l = l.replace('memory_attention/output', 'attention/encdec_output')\n",
    "#         l = l.replace('memory_attention', 'attention/encdec')\n",
    "\n",
    "#         if l not in name_to_variable:\n",
    "#             continue\n",
    "#         assignment_map[name] = name_to_variable[l]\n",
    "#         initialized_variable_names[l + ':0'] = 1\n",
    "\n",
    "#     return (assignment_map, initialized_variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = tf.trainable_variables()\n",
    "# assignment_map, initialized_variable_names = get_assignment_map_from_checkpoint(t, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://mesolitica-tpu-general/bigbird-summarization-base/model.ckpt-500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://mesolitica-tpu-general/bigbird-summarization-base/model.ckpt-500000\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_lists = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# saver = tf.train.Saver(var_list = var_lists)\n",
    "# saver.restore(sess, 'gs://mesolitica-tpu-general/bigbird-summarization-small/model.ckpt-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def cleaning(string):\n",
    "    return re.sub(r'[ ]+', ' ', unidecode(string.replace('\\n', ' '))).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "Tidak ada apa-apa mengenai mesyuarat fakulti biologi Jumaat lalu di University of Alabama di Huntsville yang membayangkan pembunuhan yang akan berlaku, Profesor Debra Moriarity memberitahu wartawan Rabu. \"Ia sebenarnya adalah satu-satunya mesyuarat fakulti yang sangat santai dan biasa,\" ahli biokim memberitahu CNN affiliate WAAY mengenai 13 orang yang duduk di sekitar meja bujur di Bilik 369 di Shelby Centre for Science and Technology. \"Peristiwa akan datang, kelas penjadualan, belanjawan. Ia sebenarnya merupakan salah satu daripada mesyuarat fakulti yang paling mudah yang kami ada.\" Antara peserta adalah Amy Bishop, seorang ahli genetik yang terlatih Harvard dengan siapa Moriarity telah membangun hubungan profesional yang dipupuk oleh hakikat bahawa kedua-dua wanita bekerja dengan budaya sel. \"Kadang-kadang anda meminjam sesuatu dari satu sama lain,\" katanya. \"Kami telah bercakap mengenai cadangan pemberian cadangan bersama.\" Selepas kira-kira sejam, sebelum 4hb, Bishop - yang baru-baru ini dinafikan tempohnya - menamatkan tenang. \"Secara tiba-tiba, dia hanya berdiri dan ditembak,\" kata Moriarity. Moriarity bertindak balas dengan cepat, menjatuhkan ke tangan dan lututnya di atas permaidani kelabu. \"Hanya jatuh ke lantai dan merangkak di bawah meja dan merangkak ke arah Amy,\" kata Moriarity, yang memberi tumpuan kepada satu perkara. \"Maksud saya, kamu merangkak di bawah meja, kamu melihat kaki orang yang menembak di atas meja, saya merangkul kakinya dan, saya tidak tahu apa yang saya fikirkan, saya tidak memikirkan apa-apa. hanya berfikir: \\'Ambil dia!\\' \"Dan dia menghalang saya. Maksud saya, dia menarik kaki kakinya percuma dan saya berada di ambang pintu dengan belakang saya jenisnya. Dan saya fikir dia cuba menembak saya, tetapi ketika saya mula berteriak kepadanya, \\'Amy, Amy, berfikir tentang cucu saya, berfikir tentang anak perempuan saya! Ini saya! Saya telah membantu anda sebelum ini; Saya akan membantu anda lagi! Jangan buat Amy ini! Jangan lakukan ini! \\'\"Uskup kemudian melangkah masuk ke dalam dewan, menunjuk pistol di Moriarity dan menarik pencetus, kata ahli biologi.\" Ia diklik, dan ia diklik lagi, dan saya merangkak kembali ke dalam bilik dan menutup pintu dan dia ditinggalkan di dalam dewan. \"Orang-orang yang terselamat itu beraksi. Satu orang mengunci pintu kayu, satu lagi mengetuk meja ke atasnya, yang lain memindahkan peti sejuk ke tempatnya untuk menghalang pintu, yang lain dipanggil 911, yang lain berpindah ke Tiga orang yang maut, tiga lagi yang cedera, dua daripadanya kekal dirawat di rumah sakit pada hari Rabu dalam keadaan kritikal, menurut seorang jurubicara Hospital Huntsville, yang ketiga dilepaskan, Moriarity, yang menyertai fakulti sekolah pada tahun 1984, mengatakan bahawa mangsa tidak menjejaskan rancangannya untuk kekal di sekolah dan dia menolak apa-apa cadangan bahawa peranannya untuk mendapatkan Bishop di luar bilik itu adalah heroik. \"Dia mengikuti saya di dewan dan kemudian senjata itu macet dan saya boleh mendapatkan ba ck di dalam bilik, \"kata Moriarity. \"Itu bukan seorang pahlawan. Itu hanya Tuhan yang melihat kamu.\" Dia berkata dia mempunyai sedikit masa untuk berfikir. \"Dari awal hingga akhirnya kami mendapat sesuatu yang tidak dapat dibendung, ia tidak boleh melebihi 20 saat,\" katanya. Moriarity terus menolak cadangan bahawa apa-apa boleh dilakukan untuk melindungi mangsa. \"Tidak ada cara untuk menjangka ini,\" katanya. \"Dan tiada apa yang dapat dilakukan untuk menghentikannya, semuanya berlaku terlalu pantas.\" Dan dia bimbang bahawa apa-apa cubaan untuk mengetatkan keselamatan boleh membawa kesan negatif. \"Ada kejahatan di dunia, malangnya orang baik disakiti oleh itu, tetapi universiti adalah tempat pemikiran bebas dan kebebasan untuk meneroka idea-idea dan mencari pengetahuan baru dan anda tidak mahu meletakkan sesuatu di tempat yang meredakannya. \" Moriarity kembali ke pejabatnya pada hari Rabu dan berkata dia merancang untuk meneruskan pengajaran minggu depan. Dia meramalkan bahawa, dengan bantuan ubat anti-kecemasan, dia akan dapat tidur malam Rabu. \"Saya telah bercakap dengan keluarga dan kawan-kawan dan hanya mendapatkan sokongan mereka membantu anda menanganinya,\" katanya. \"Saya fikir sekarang kebanyakan dari kita mahu kembali ke sana dan mendapatkan sesuatu yang berlaku, membuat rancangan untuk siapa yang akan menutup kelas.\" Satu perkhidmatan peringatan untuk menghormati kehidupan orang mati - ahli fakulti Maria Davis, Adriel Johnson dan Gopi Podila - akan diadakan pada hari Jumaat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "string2 = \"\"\"\n",
    "Gabungan parti Warisan, Pakatan Harapan, dan Upko hari ini mendedahkan calon-calon masing-masing untuk pilihan raya negeri Sabah, tetapi ketika pengumuman itu berlangsung, perwakilan PKR di dewan itu dilihat ‘gelisah’ seperti ‘tidak senang duduk’.\n",
    "\n",
    "Sekumpulan anggota PKR kemudian dilihat meninggalkan dewan di Pusat Konvensyen Antarabangsa Sabah di Kota Kinabalu selepas berbincang dengan ketua PKR Sabah Christina Liew.\n",
    "\n",
    "Semakan senarai-senarai calon berkenaan mendapati PKR hanya memperolehi separuh daripada jumlah kerusi yang diharapkan.\n",
    "\n",
    "Semalam, PKR Sabah mengumumkan akan bertanding di 14 kerusi tetapi ketika Presiden Warisan Shafie Apdal mengumumkan calon gabungan tersebut hari ini, PKR hanya diberikan tujuh kerusi untuk bertanding.\n",
    "\n",
    "Kerusi yang diberikan adalah Api-Api, Inanam, Tempasuk, Tamparuli, Matunggong, Klias, dan Sook.\n",
    "\n",
    "Klias dan Sook adalah dua kerusi yang diberikan kepada PKR, sementara lima kerusi selebihnya pernah ditandingi oleh PKR pada pilihan raya umum 2018.\n",
    "\n",
    "Dalam pengumuman PKR Sabah semalam, parti itu menjangkakan Warisan akan turut menyerahkan kerusi Kemabong, Membakut, dan Petagas kepada mereka.\n",
    "\n",
    "Walau bagaimanapun, Warisan menyerahkan kerusi Kemabong kepada Upko dan mengekalkan bertanding untuk kerusi Membakut dan Petagas.\n",
    "\n",
    "PKR juga menuntut empat daripada 13 kerusi baru yang diperkenalkan iaitu Segama, Limbahau, Sungai Manila, dan Pintasan tetapi Warisan membolot semua kerusi itu.\n",
    "\n",
    "Sebagai pertukaran untuk kerusi yang diintainya, PKR bersedia untuk menyerahkan kerusi Kadaimaian, Kuala Penyu, dan Karanaan. Namun, ini dijangka tidak akan berlaku memandangkan parti tersebut tidak berpuas hati dengan agihan kerusi seperti yang diharapkan itu.\n",
    "\n",
    "Selepas perwakilan dari PKR dan Liew keluar dari dewan tersebut, wartawan kemudian menyusuri Liew untuk mendapatkan penjelasannya.\n",
    "\n",
    "Walau bagaimanapun, Liew enggan memberikan sebarang komen dan berkata bahawa dia ingin ke tandas.\n",
    "\n",
    "Liew dan perwakilan PKR kemudian tidak kembali ke dalam dewan tersebut.\n",
    "\n",
    "Apabila calon pilihan raya yang diumumkan diminta naik ke atas pentas untuk sesi bergambar, Liew tidak kelihatan.\n",
    "\n",
    "Bilangan kerusi yang ditandingi oleh PKR kali ini hanya kurang satu kerusi daripada yang ditandingi parti itu pada PRU 2018.\n",
    "\n",
    "Dalam perkembangan berkaitan, DAP dan Amanah dikatakan tidak mempunyai sebarang masalah dengan kerusi yang diberikan untuk PRN Sabah.\n",
    "\n",
    "Sementara itu, Presiden Upko Madius Tangau enggan mengulas adakah dia berpuas hati dengan agihan kerusi tersebut. Madius kekal di majlis tersebut sehingga ia berakhir.\n",
    "\n",
    "Partinya diberikan 12 kerusi, iaitu lebih tujuh kerusi berbanding PRU lalu.\n",
    "\n",
    "DAP dan Amanah akan bertanding di bawah logo Warisan sementara PKR dan Upko akan menggunakan logo masing-masing.\n",
    "\n",
    "DAP akan bertanding di tujuh kerusi, jumlah yang sama seperti yang mereka tandingi pada PRU lalu, sementara Amanah diberi satu kerusi.\n",
    "\n",
    "Warisan akan bertanding sebanyak 54 kerusi.\n",
    "\n",
    "Perkembangan terbaru ini mungkin mencetuskan pergeseran di antara PKR dan Warisan. PKR boleh memilih untuk bertanding di lebih banyak kerusi daripada 14 yang dituntutnya manakala Warisan juga boleh bertanding di kerusi sekutunya.\n",
    "\n",
    "Barisan pemimpin tertinggi PKR dan Warisan hanya mempunyai dua hari sebelum hari penamaan calon pada Sabtu untuk mengurangkan pergeseran.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "string3 = \"\"\"\n",
    "Penubuhan universiti sukan seperti diutarakan Ketua Unit Sukan Kementerian Pengajian Tinggi, Dr Pekan Ramli dan disokong Pakar Pembangunan Sukan dan Reakreasi Luar, Universiti Pendidikan Sultan Idris (UPSI), Prof Dr Md Amin Md Taaf seperti disiarkan akhbar ini, memberikan sinar harapan kepada kewujudan institusi sedemikian.\n",
    "\n",
    "Ia menjadi impian atlet negara untuk mengejar kejayaan dalam bidang sukan dan kecemerlangan dalam akademik untuk menjamin masa depan lebih baik apabila bersara daripada arena sukan kelak.\n",
    "\n",
    "Pelbagai pandangan, idea, kaedah, bukti dan cadangan dilontarkan pakar berikutan pentingnya universiti sukan yang akan memberi impak besar sama ada pada peringkat kebangsaan mahupun antarabangsa.\n",
    "\n",
    "Negara lain sudah lama meraih laba dengan kewujudan universiti sukan seperti China, Korea, Japan, Taiwan, India dan Vietnam. Mereka menghasilkan atlet universiti yang mempamerkan keputusan cemerlang pada peringkat tinggi seperti Sukan Olimpik, Kejohanan Dunia dan Sukan Asia.\n",
    "\n",
    "Justeru, kejayaan mereka perlu dijadikan rujukan demi memajukan sukan tanah air. Jika kita merujuk pendekatan Asia, kewujudan universiti sukan penting dan memberi kesan positif dalam melonjakkan prestasi sukan lebih optimum.\n",
    "\n",
    "Namun, jika kita melihat pendekatan Eropah, universiti sukan bukan antara organisasi atau institusi penting yang diberi perhatian dalam menyumbang kepada pemenang pingat.\n",
    "\n",
    "Antara isu dalam universiti sukan ialah kos tinggi, lokasi, prasarana sukan, pertindihan kursus dengan universiti sedia ada dan impak terhadap dunia sukan negara hingga mengundang persoalan kewajaran dan kerelevanan penubuhannya.\n",
    "\n",
    "Namun sebagai bekas atlet memanah negara dan Olympian (OLY) di Sukan Olimpik 2004 di Athens, Greece serta bekas pelajar Sekolah Sukan Bukit Jalil hingga berjaya dalam dunia akademik, saya mendapati terdapat beberapa faktor sering menjadi halangan dalam rutin harian mereka.\n",
    "\n",
    "Antaranya, faktor masa yang terpaksa bergegas menghadiri kuliah selepas tamat sesi latihan yang mengambil masa 15 hingga 20 minit dengan menunggang motosikal; kereta (20-30 minit) atau pengangkutan disediakan Majlis Sukan Negara (MSN) ke Universiti Putra Malaysia (UPM).\n",
    "\n",
    "Jika mereka menuntut di Universiti Teknologi MARA (UiTM) atau Universiti Malaya (UM), ia mungkin lebih lama.\n",
    "\n",
    "Walaupun di universiti tersedia dengan kemudahan kolej dan kemudahan sukan, mereka memilih pulang ke MSN untuk menjalani latihan bersama pasukan dan jurulatih di padang atau gelanggang latihan rasmi.\n",
    "\n",
    "Ini berlanjutan selagi bergelar atlet negara yang perlu memastikan prestasi sentiasa meningkat dari semasa ke semasa tanpa mengabaikan tugas sebagai pelajar.\n",
    "\n",
    "Alangkah baiknya jika sebahagian Sekolah Sukan Bukit Jalil itu sendiri dijadikan Kolej atau Universiti Sukan Malaysia kerana lengkap dari segi kemudahan prasarana sukannya dan proses pengajaran dan pembelajaran (PdP) dalam bidang Sains Sukan, Kejurulatihan, Pendidikan Jasmani dan setaraf dengannya.\n",
    "\n",
    "Pengambilan setiap semester pula hanya terhad kepada atlet berstatus kebangsaan dan antarabangsa sahaja supaya hasrat melahirkan lebih ramai atlet bertaraf Olimpik mudah direalisasikan.\n",
    "\n",
    "Contohnya, bekas atlet lompat bergalah negara, Roslinda Samsu yang juga pemenang pingat perak Sukan Asia Doha 2006 dan Penerima Anugerah Khas Majlis Anugerah Sukan KPT 2012, terpaksa mengambil masa lebih kurang sembilan tahun untuk menamatkan ijazah Sarjana Muda Pendidikan Jasmani di UPM sepanjang 14 tahun terbabit dalam sukan olahraga.\n",
    "\n",
    "Sepanjang tempoh bergelar atlet kebangsaan dan mahasiswa, beliau juga memenangi pingat Emas Sukan SEA empat siri berturut-turut pada 2005, 2007, 2009 dan 2011.\n",
    "\n",
    "Begitu juga atlet kebangsaan seperti Leong Mun Yee (UPM); Pandalela Renong (UM); Bryan Nickson Lomas (UM); Cheng Chu Sian (UPM); Marbawi Sulaiman (UiTM) dan Norasheela Khalid (UPM).\n",
    "\n",
    "Jika disenaraikan, mungkin lebih ramai lagi. Namun, pernah terlintas di fikiran mengapa hanya atlet dari sukan terjun yang dapat memenangi pingat di Sukan Olimpik? Bagaimana dengan atlet lain yang juga layak secara merit? Apakah kekangan atau masalah dihadapi sebagai atlet dan mahasiswa?\n",
    "\n",
    "Adakah kewujudan universiti sukan akan memberi impak besar kepada kemajuan sukan negara? Jika dirancang dan diatur dengan cekap dan sistematik, ia perkara tidak mustahil dicapai.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder.encode(f'{cleaning(string2)}') + [1]\n",
    "s = pad_sequences([encoded], padding='post', maxlen = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 5.01 s, total: 3min 27s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "l = sess.run(logits, feed_dict = {X: s, top_p: 0.0, temperature: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Kerusi di Sabah tidak akan ditandingi oleh gabungan parti itu, tetapi akan menjadi yang tertinggi di negeri ini, menurut beberapa anggota Amanah. Keputusan mengejutkan itu berlaku setelah semua orang yang bertanding di kerusi itu meninggalkan parti itu pada pilihan raya umum lalu . Parti itu pada asalnya hanya mempunyai 54 perwakilan, tetapi akan mendapat jumlah perwakilan, sementara hanya hanya satu daripada tujuh kerusi yang bertanding.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode(l[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/model.ckpt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top_p',\n",
       " 'temperature',\n",
       " 'pegasus/embeddings/word_embeddings',\n",
       " 'pegasus/embeddings/position_embeddings',\n",
       " 'Placeholder',\n",
       " 'pegasus/encoder/layer_0/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_0/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_0/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_0/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_0/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_0/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_0/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_0/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_0/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_0/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_0/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_0/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_0/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_0/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_0/output/dense/bias',\n",
       " 'pegasus/encoder/layer_1/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_1/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_1/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_1/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_1/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_1/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_1/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_1/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_1/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_1/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_1/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_1/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_1/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_1/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_1/output/dense/bias',\n",
       " 'pegasus/encoder/layer_2/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_2/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_2/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_2/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_2/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_2/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_2/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_2/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_2/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_2/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_2/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_2/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_2/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_2/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_2/output/dense/bias',\n",
       " 'pegasus/encoder/layer_3/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_3/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_3/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_3/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_3/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_3/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_3/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_3/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_3/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_3/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_3/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_3/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_3/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_3/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_3/output/dense/bias',\n",
       " 'pegasus/encoder/layer_4/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_4/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_4/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_4/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_4/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_4/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_4/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_4/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_4/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_4/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_4/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_4/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_4/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_4/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_4/output/dense/bias',\n",
       " 'pegasus/encoder/layer_5/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_5/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_5/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_5/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_5/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_5/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_5/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_5/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_5/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_5/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_5/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_5/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_5/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_5/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_5/output/dense/bias',\n",
       " 'pegasus/encoder/layer_6/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_6/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_6/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_6/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_6/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_6/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_6/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_6/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_6/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_6/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_6/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_6/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_6/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_6/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_6/output/dense/bias',\n",
       " 'pegasus/encoder/layer_7/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_7/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_7/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_7/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_7/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_7/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_7/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_7/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_7/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_7/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_7/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_7/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_7/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_7/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_7/output/dense/bias',\n",
       " 'pegasus/encoder/layer_8/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_8/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_8/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_8/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_8/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_8/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_8/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_8/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_8/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_8/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_8/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_8/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_8/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_8/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_8/output/dense/bias',\n",
       " 'pegasus/encoder/layer_9/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_9/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_9/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_9/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_9/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_9/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_9/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_9/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_9/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_9/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_9/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_9/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_9/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_9/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_9/output/dense/bias',\n",
       " 'pegasus/encoder/layer_10/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_10/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_10/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_10/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_10/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_10/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_10/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_10/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_10/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_10/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_10/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_10/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_10/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_10/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_10/output/dense/bias',\n",
       " 'pegasus/encoder/layer_11/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_11/attention/self/query/kernel',\n",
       " 'pegasus/encoder/layer_11/attention/self/key/kernel',\n",
       " 'pegasus/encoder/layer_11/attention/self/value/kernel',\n",
       " 'pegasus/encoder/layer_11/attention/self/Softmax',\n",
       " 'pegasus/encoder/layer_11/attention/self/Softmax_1',\n",
       " 'pegasus/encoder/layer_11/attention/self/Softmax_2',\n",
       " 'pegasus/encoder/layer_11/attention/self/Softmax_3',\n",
       " 'pegasus/encoder/layer_11/attention/self/Softmax_4',\n",
       " 'pegasus/encoder/layer_11/attention/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_11/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/encoder/layer_11/intermediate/dense/kernel',\n",
       " 'pegasus/encoder/layer_11/intermediate/dense/bias',\n",
       " 'pegasus/encoder/layer_11/output/dense/kernel',\n",
       " 'pegasus/encoder/layer_11/output/dense/bias',\n",
       " 'pegasus/encoder/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_0/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_0/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_0/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_0/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_0/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_0/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_0/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_0/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_0/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_0/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_0/output/dense/bias',\n",
       " 'pegasus/decoder/layer_1/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_1/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_1/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_1/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_1/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_1/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_1/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_1/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_1/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_1/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_1/output/dense/bias',\n",
       " 'pegasus/decoder/layer_2/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_2/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_2/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_2/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_2/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_2/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_2/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_2/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_2/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_2/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_2/output/dense/bias',\n",
       " 'pegasus/decoder/layer_3/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_3/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_3/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_3/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_3/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_3/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_3/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_3/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_3/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_3/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_3/output/dense/bias',\n",
       " 'pegasus/decoder/layer_4/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_4/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_4/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_4/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_4/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_4/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_4/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_4/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_4/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_4/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_4/output/dense/bias',\n",
       " 'pegasus/decoder/layer_5/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_5/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_5/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_5/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_5/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_5/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_5/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_5/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_5/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_5/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_5/output/dense/bias',\n",
       " 'pegasus/decoder/layer_6/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_6/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_6/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_6/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_6/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_6/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_6/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_6/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_6/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_6/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_6/output/dense/bias',\n",
       " 'pegasus/decoder/layer_7/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_7/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_7/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_7/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_7/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_7/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_7/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_7/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_7/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_7/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_7/output/dense/bias',\n",
       " 'pegasus/decoder/layer_8/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_8/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_8/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_8/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_8/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_8/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_8/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_8/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_8/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_8/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_8/output/dense/bias',\n",
       " 'pegasus/decoder/layer_9/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_9/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_9/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_9/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_9/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_9/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_9/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_9/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_9/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_9/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_9/output/dense/bias',\n",
       " 'pegasus/decoder/layer_10/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_10/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_10/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_10/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_10/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_10/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_10/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_10/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_10/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_10/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_10/output/dense/bias',\n",
       " 'pegasus/decoder/layer_11/attention/self/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_11/attention/self/query/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/self/key/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/self/value/kernel',\n",
       " 'pegasus/while/decoder/layer_11/attention/self/Softmax',\n",
       " 'pegasus/decoder/layer_11/attention/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/encdec/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_11/attention/encdec/query/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/encdec/key/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/encdec/value/kernel',\n",
       " 'pegasus/decoder/layer_11/attention/encdec_output/dense/kernel',\n",
       " 'pegasus/decoder/layer_11/intermediate/LayerNorm/gamma',\n",
       " 'pegasus/decoder/layer_11/intermediate/dense/kernel',\n",
       " 'pegasus/decoder/layer_11/intermediate/dense/bias',\n",
       " 'pegasus/decoder/layer_11/output/dense/kernel',\n",
       " 'pegasus/decoder/layer_11/output/dense/bias',\n",
       " 'pegasus/decoder/LayerNorm/gamma',\n",
       " 'pegasus/while/decoder/layer_0/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_1/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_2/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_3/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_4/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_5/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_6/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_7/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_8/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_9/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_10/attention/self/Softmax_1',\n",
       " 'pegasus/while/decoder/layer_11/attention/self/Softmax_1',\n",
       " 'pegasus/logits',\n",
       " 'logits']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'top_p' in n.name\n",
    "        or 'temperature' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name\n",
    "        or 'self/Softmax' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'gradients' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-9a7215a4e58a>:23: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-9a7215a4e58a>:23: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 366 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 366 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 366 variables to const ops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted 366 variables to const ops.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24644 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('output', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-f9c2d7850f78>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-f9c2d7850f78>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "pb = 'output/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "        \n",
    "inputs = ['Placeholder', 'top_p', 'temperature']\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                       inputs,\n",
    "                                       ['logits'], transforms)\n",
    "\n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename, **kwargs):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # https://github.com/onnx/tensorflow-onnx/issues/77#issuecomment-445066091\n",
    "    # to fix import T5\n",
    "    for node in graph_def.node:\n",
    "        if node.op == 'RefSwitch':\n",
    "            node.op = 'Switch'\n",
    "            for index in xrange(len(node.input)):\n",
    "                if 'moving_' in node.input[index]:\n",
    "                    node.input[index] = node.input[index] + '/read'\n",
    "        elif node.op == 'AssignSub':\n",
    "            node.op = 'Sub'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'AssignAdd':\n",
    "            node.op = 'Add'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'Assign':\n",
    "            node.op = 'Identity'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "            if 'validate_shape' in node.attr:\n",
    "                del node.attr['validate_shape']\n",
    "            if len(node.input) == 2:\n",
    "                node.input[0] = node.input[1]\n",
    "                del node.input[1]\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "top_p = g.get_tensor_by_name('import/top_p:0')\n",
    "temperature = g.get_tensor_by_name('import/temperature:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 43s, sys: 18.6 s, total: 8min 2s\n",
      "Wall time: 50.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'- Kerusi di kerusi Parlimen di Sabah tidak akan ditandingi oleh Presiden PKR, Liew Junung, yang akan meninggalkan tujuh kerusi yang ada di negeri ini. Tetapi adakah ia akan kekal? \"Amaran hati saya bahawa saya tidak berpuas hati dengan jumlah kerusi Warisan untuk menyerahkan kerusi itu kepada saya,\" kata Liew kepada Madius Tangau. \"Masih ada satu kerusi yang boleh diberikan, tetapi saya tidak berpuas hati dengan jumlah kerusi yang diberikan oleh PKR. \" Kerusi itu ditandingi oleh gabungan tujuh kerusi dan dewan yang ada di negeri ini . Tetapi, seperti yang dinyatakan oleh Madius Tangau, \"penduduk PKR\" tidak berpuas hati dengan jumlah kerusi yang diberikan oleh PKR. \"Amaran itu berlaku ketika parti itu bersiap untuk meninggalkan pilihan raya umum ke-14 tetapi kemudian akan menyerahkan kerusi itu kepada pemimpin yang akan ditandingi oleh DAP,\" kata Madius Tangau.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "l = test_sess.run(logits, feed_dict = {x: s, top_p: 0.0, temperature: 0.0})\n",
    "encoder.decode([i for i in l[0].tolist() if i > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = load_graph('output/frozen_model.pb.quantized')\n",
    "# x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "# top_p = g.get_tensor_by_name('import/top_p:0')\n",
    "# temperature = g.get_tensor_by_name('import/temperature:0')\n",
    "# logits = g.get_tensor_by_name('import/logits:0')\n",
    "# test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# l = test_sess.run(logits, feed_dict = {x: s, top_p: 0.0, temperature: 0.0})\n",
    "# encoder.decode([i for i in l[0].tolist() if i > 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
